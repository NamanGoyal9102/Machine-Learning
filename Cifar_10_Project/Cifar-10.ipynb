{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "329907d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac9d7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve (\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"a.tar.gz\")\n",
    "import tarfile\n",
    "tar = tarfile.open(\"a.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4379929",
   "metadata": {},
   "source": [
    "# Loading the data from the dataset into the train_x and test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6777660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    " '''Load byte data from file'''\n",
    " with open(file, 'rb') as f:\n",
    "  data = pickle.load(f, encoding='latin-1')\n",
    "  return data\n",
    "\n",
    "\n",
    "def load_cifar10_data(data_dir):\n",
    " '''Return train_data, train_labels, test_data, test_labels\n",
    " The shape of data is 32 x 32 x3'''\n",
    " train_data = None\n",
    " train_labels = []\n",
    "\n",
    " for i in range(1, 6):\n",
    "  data_dic = unpickle(data_dir + \"/data_batch_{}\".format(i))\n",
    "  if i == 1:\n",
    "   train_data = data_dic['data']\n",
    "  else:\n",
    "   train_data = np.vstack((train_data, data_dic['data']))\n",
    "  train_labels += data_dic['labels']\n",
    "\n",
    " test_data_dic = unpickle(data_dir + \"/test_batch\")\n",
    " test_data = test_data_dic['data']\n",
    " test_labels = test_data_dic['labels']\n",
    "\n",
    " train_data = train_data.reshape((len(train_data), 3, 32, 32))\n",
    " train_data = np.rollaxis(train_data, 1, 4)\n",
    " train_labels = np.array(train_labels)\n",
    "\n",
    " test_data = test_data.reshape((len(test_data), 3, 32, 32))\n",
    " test_data = np.rollaxis(test_data, 1, 4)\n",
    " test_labels = np.array(test_labels)\n",
    "\n",
    " return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "data_dir = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(data_dir)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c62e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304a620",
   "metadata": {},
   "source": [
    "# Reshaping the data into a 2D numpy array of size 5000 X 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d70f1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.reshape(train_data.shape[0],-1)\n",
    "x_test = test_data.reshape(test_data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0405e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1309dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_scaled = sc.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51e43a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77b8b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff81ae4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0011e0",
   "metadata": {},
   "source": [
    "# Performing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f325a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.20557381e+01,  1.22849042e+01,  8.96135095e+00, ...,\n",
       "         5.39089318e-03,  5.58180986e-04, -2.60774472e-03],\n",
       "       [ 4.01354905e+00, -5.04915634e+00,  2.53958923e+01, ...,\n",
       "        -1.20631185e-03, -1.55110611e-03,  4.72296446e-03],\n",
       "       [ 2.11123034e+01, -4.76871967e+01, -1.25735508e+01, ...,\n",
       "        -1.95641086e-03, -1.98150370e-03,  1.93502441e-03],\n",
       "       ...,\n",
       "       [-5.79011324e+00, -4.49244141e+01, -4.24725698e+00, ...,\n",
       "         5.51117587e-03, -5.06192238e-03,  1.70515002e-03],\n",
       "       [ 4.23917856e+01, -1.65511813e+01,  2.22660304e+01, ...,\n",
       "        -1.11556537e-03, -1.43552607e-03,  6.52224660e-03],\n",
       "       [ 1.30753871e+01, -3.19221852e+00, -1.73174907e+01, ...,\n",
       "        -2.31232834e-03,  2.07327476e-03,  1.24354774e-03]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(x_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0787a6",
   "metadata": {},
   "source": [
    "# Calculating amount of Features to retain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2685ea9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating optimal k to have 0.95 variance\n",
    "\n",
    "k = 0\n",
    "total = sum(pca.explained_variance_)\n",
    "current_sum = 0\n",
    "\n",
    "while(current_sum / total < 0.99):\n",
    "    current_sum += pca.explained_variance_[k]\n",
    "    k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e93e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cifar = PCA(n_components=k,whiten=True)\n",
    "x_train_pca_cifar = pca_cifar.fit_transform(x_train_scaled)\n",
    "x_test_pca_cifar = pca_cifar.transform(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750ccde",
   "metadata": {},
   "source": [
    "# Applying Logistic Regression for the Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "346aedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d9d81f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train_pca_cifar, y_train)\n",
    "y_pred_lr = lr.predict(x_test_pca_cifar)\n",
    "logistic_regression_score = accuracy_score(y_test, y_pred_lr)\n",
    "logistic_regression_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066dd20",
   "metadata": {},
   "source": [
    "# Applying Support Vector Machines for the Prediction Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(C= 100, gamma=0.005)\n",
    "svc.fit(x_train_pca_cifar,y_train)\n",
    "y_pred_svm = svc.predict(x_test_pca_cifar)\n",
    "svc_score = accuracy_score(y_test, y_pred_svm)\n",
    "svc_score\n",
    "print(\"time is :\",(time.time()-start)/60,\" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
